---
title: "Assignment 2 by Félix Báez-Santiago and Gloria Zhou"
subtitle: "Due at 11:59pm on October 1."
format: html
editor: visual
---

## 

You may work in pairs or individually for this assignment. Make sure you join a group in Canvas if you are working in pairs. Turn in this assignment as an HTML or PDF file to ELMS. Make sure to include the R Markdown or Quarto file that was used to generate it.

```{r, message = FALSE}
library(tidyverse)
library(gtrendsR)
library(censusapi)
library(dplyr)
```

In this assignment, you will pull from APIs to get data from various data sources and use your data wrangling skills to use them all together. You should turn in a report in PDF or HTML format that addresses all of the questions in this assignment, and describes the data that you pulled and analyzed. You do not need to include full introduction and conclusion sections like a full report, but you should make sure to answer the questions in paragraph form, and include all relevant tables and graphics.

Whenever possible, use piping and `dplyr`. Avoid hard-coding any numbers within the report as much as possible.

## Pulling from APIs

Our first data source is the Google Trends API. Suppose we are interested in the search trends for `crime` and `loans` in Illinois in the year 2020. We could find this using the following code:

```{r}
res <- gtrends(c("crime", "loans"), 
               geo = "US-IL", 
               time = "2020-01-01 2020-12-31", 
               low_search_volume = TRUE)
plot(res)
interest_over_time <- data.frame(res$interest_over_time)
interest_by_dma <- data.frame(res$interest_by_dma)
interest_by_city <- data.frame(res$interest_by_city)
head(interest_over_time)
head(interest_by_dma)
head(interest_by_city)
```

Answer the following questions for the keywords "crime" and "loans".

-   Find the mean, median and variance of the search hits for the keywords.

```{r}
interest_over_time %>%
  na.omit() %>%
  group_by(keyword) %>%
  summarise(
    search_hits_mean = mean(hits),
    search_hits_median = median(hits),
    search_hits_variance = var(hits)
  )

interest_by_dma %>%
  na.omit() %>%
  group_by(keyword) %>%
  summarise(
    search_hits_mean = mean(hits),
    search_hits_median = median(hits),
    search_hits_variance = var(hits)
  )

interest_by_city %>%
  na.omit() %>%
  group_by(keyword) %>%
  summarise(
    search_hits_mean = mean(hits),
    search_hits_median = median(hits),
    search_hits_variance = var(hits)
  )
```

*Within `interest_over_time`, the mean of `crim` is 56.887 and the median is 57, with the variance of 85.295; while the mean of `loans` is 68.906 and the median is 66, with the variance of 105.241. Within `interest_by_dma`, the mean of `crim` is 85.8 and the median is 87.0, with the variance of 75.733; while the mean of `loans` is 82.3 and the median is 80.5, with the variance of 81.789. Within `interest_by_city`, the mean of `crim` is 65.821 and the median is 61, with the variance of 151.625; while the mean of `loans` is 59.556 and the median is 57, with the variance of 97.454.*

-   Which cities (locations) have the highest search frequency for `loans`? Note that there might be multiple rows for each city if there were hits for both "crime" and "loans" in that city. It might be easier to answer this question if we had the search hits info for both search terms in two separate variables. That is, each row would represent a unique city.

    ```{r}
    highest_search_frequency <- interest_by_city %>% # search in city dataset
      na.omit() %>% # remove null values
      filter(keyword == "loans") %>% # filter for only rows searches for loans
      transmute(
        location,
        freq = hits / sum(hits)
        ) %>% #  select city and create frequency variable
      arrange(desc(freq)) # arrange in descending order of search frequency 
    head(highest_search_frequency, 10)

    ggplot(highest_search_frequency, 
           aes(x = reorder(location, -freq), 
               y = freq)) +
      geom_bar(stat = "identity", fill = "steelblue") +
      labs(x = "City in Illinois (Location)", 
           y = "Search Frequency for Loan", 
           title = "Which cities have the highest search frequency for loans?") +
      theme_minimal() +
      theme(axis.text.x = element_text(angle = 45, hjust = 1))
    ```

    *Long Lake has the highest search frequency for `loans`.*

-   Is there a relationship between the search intensities between the two keywords we used?

```{r}
correlation <- cor(interest_over_time$hits[interest_over_time$keyword == "crime"],
                   interest_over_time$hits[interest_over_time$keyword == "loans"])
round(correlation,3)
```

*There is a very negative weak correlation between the two keywords we used.*

Repeat the above for keywords related to covid. Make sure you use multiple keywords like we did above. Try several different combinations and think carefully about words that might make sense within this context.

```{r}
#Fetch Google Trends data for Covid related keywords

covid_res <- gtrends(c("unemployment", "death"),
                      geo = "US-CA",
                      time = "2020-01-01 2020-12-31",
                      low_search_volume = TRUE)
plot(covid_res)
covid_interest_over_time <- data.frame(covid_res$interest_over_time)
covid_interest_by_dma <- data.frame(covid_res$interest_by_dma)
covid_interest_by_city <- data.frame(covid_res$interest_by_city)
head(covid_interest_over_time)
head(covid_interest_by_dma)
head(covid_interest_by_city)
```

-   Find the the mean of `crim` is , median and variance of the search hits for the covid keywords.

```{r}
# Calculate mean, median, and variance for each keyword
covid_interest_over_time %>%
  na.omit() %>%
  group_by(keyword) %>%
  summarise(
    search_hits_mean = mean(hits),
    search_hits_median = median(hits),
    search_hits_variance = var(hits)
  )

covid_interest_by_dma %>%
  na.omit() %>%
  group_by(keyword) %>%
  summarise(
    search_hits_mean = mean(hits),
    search_hits_median = median(hits),
    search_hits_variance = var(hits)
  )

covid_interest_by_city %>%
  na.omit() %>%
  group_by(keyword) %>%
  summarise(
    search_hits_mean = mean(hits),
    search_hits_median = median(hits),
    search_hits_variance = var(hits)
  )
```

*Within `covid_interest_over_time`, the mean of the search hits for the keyword `death` is 46.264, and the median is 44, with the variance of 84.544; while the mean of the search hits for the keyword `unemployment` is 35.717, and the median is 37, with variance of 577.476.Within `covid_interest_by_dma`, the mean of the search hits for the keyword `death` is 84.286, and the median is 85, with the variance of 47.604; while the mean of the search hits for the keyword `unemployment` is 52.071, and the median is 46, with variance of 412.687. Within `covid_interest_by_city`, the mean of the search hits for the keyword `death` is 20.116, and the median is 17, with the variance of 158.819; while the mean of the search hits for the keyword `unemployment` is 74.308, and the median is 70.5, with variance of 87.742.*

-   Which cities (locations) have the highest search frequency for death? Note that there might be multiple rows for each city if there were hits for both "unemployment" and "death" in that city. It might be easier to answer this question if we had the search hits info for both search terms in two separate variables. That is, each row would represent a unique city.

```{r}
# Find the city with the highest search frequency for each keyword###
highest_death_frequency <- covid_interest_by_city %>% # search in city dataset
  na.omit() %>% # remove null values
  filter(keyword == "death") %>% # filter for only rows searches for loans
  transmute(
    location,
    freq = hits / sum(hits)
    ) %>% #  select city and create frequency variable
  arrange(desc(freq)) # arrange in descending order of search frequency 
head(highest_death_frequency, 10)

ggplot(highest_death_frequency, 
       aes(x = reorder(location, -freq), 
           y = freq)) +
  geom_bar(stat = "identity", fill = "darkgreen") +
  labs(x = "City in California (Location)", 
       y = "Search Frequency for Deaths", title = "Which cities have the highest search frequency for deaths related to Covid?") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
# If any correlation between search intensities of COVID related keywords
correlation_covid <- cor(covid_interest_over_time$hits[covid_interest_over_time$keyword == "unemployment"],
                       covid_interest_over_time$hits[covid_interest_over_time$keyword == "death"])
round(correlation_covid, 3)
```

*There is a positive moderate correlation between these two keywords we used.*

## Google Trends + ACS

Now lets add another data set. The `censusapi` package provides a nice R interface for communicating with this API. However, before running queries we need an access key. This (easy) process can be completed here:

<https://api.census.gov/data/key_signup.html>

Once you have an access key, save it as a text file, then read this key in the `cs_key` object. We will use this object in all following API queries. Note that I called my text file `census-key.txt` – yours might be different!

`{library(censusapi)} cs_key <- read_file("census-key.txt")`

In the following, we request basic socio-demographic information (population, median age, median household income, income per capita) for cities and villages in the state of Illinois. Documentation for the 5-year ACS API can be found here: <https://www.census.gov/data/developers/data-sets/acs-5year.html>. The information about the variables used here can be found here: <https://api.census.gov/data/2022/acs/acs5/variables.html>.

```{r}
cs_key <- read_file("census-key.txt")
acs_il <- getCensus(name = "acs/acs5",
                    vintage = 2020, 
                    vars = c("NAME", 
                             "B01001_001E", 
                             "B06002_001E", 
                             "B19013_001E", 
                             "B19301_001E"), 
                    region = "place:*", 
                    regionin = "state:17",
                    key = cs_key)
head(acs_il)
```

Convert values that represent missings to NAs.

```{r}
acs_il[acs_il == -666666666] <- NA
sum(is.na(acs_il))
```

Now, it might be useful to rename the socio-demographic variables (`B01001_001E` etc.) in our data set and assign more meaningful names.

```{r}
acs_il <-
  acs_il %>%
  rename(pop = B01001_001E,
         age = B06002_001E,
         hh_income = B19013_001E,
         income = B19301_001E)
names(acs_il)
```

It seems like we could try to use this location information listed above to merge this data set with the Google Trends data. However, we first have to clean `NAME` so that it has the same structure as `location` in the search interest by city data. Add a new variable `location` to the ACS data that only includes city names.

```{r}
# Add a new variable `location ` to the ACS data that only includes city names
acs_il$location <- acs_il$NAME
acs_il$location <- sapply(strsplit(as.character(acs_il$location), ","), '[',1)
# Use gsub() to remove suffixes like 'village', 'city', 'town', 'CDP'
acs_il$location <- gsub("\\s+(village|city|town|CDP)$", "", acs_il$location)

# Reorder columns 
acs_il <- acs_il %>%
  select(state, place, location, pop, age, hh_income, income)
acs_il
```

Answer the following questions with the "crime" and "loans" Google trends data and the ACS data.

-   First, check how many cities don't appear in both data sets, i.e. cannot be matched. Then, create a new data set by joining the Google Trends and the ACS data. Keep only cities that appear in both data sets.

```{r}
# Fetch Google trends data
acs_cities <- unique(acs_il$location)
gtrends_cities <- unique(interest_by_city$location)
unmatched_cities <- setdiff(acs_cities, gtrends_cities)

head(unmatched_cities)
num_unmatched <- length(unmatched_cities)
num_unmatched
```

*There are* `num_unmatched` *cities don't appear in both data sets.*

```{r}
# Create a new data set by joining the Google Trends and the ACS data
# Keep 
merge_both <- inner_join(acs_il, interest_by_city, by = "location")
merge_both
```

-   Compute the mean of the search popularity for both keywords for cities that have an above average median household income and for those that have an below average median household income. When building your pipe, start with creating the grouping variable and then proceed with the remaining tasks. What conclusions might you draw from this?

    ```{r}
    # mean_keywords <- merge_both %>%
    #   mutate(income = ifelse(hh_income > mean(hh_income, na.rm = TRUE), "Above Average", "Below Average")) %>%
    #   group_by(income, keyword) %>%
    #   summarise(mean_hits = mean(hits, na.rm = TRUE),
    #             .groups = 'drop')
    # 
    # mean_keywords
    ```

```{r}
library(ggplot2)
# 
# #scatterplot of median household income vs. search popularity
# ggplot(merge_both, aes(x = hh_income,
#                        y = hits,
#                        color = "red"") +
#          geom_point() +
#          labs(title = "Relationship between \nMedian Household Income and Search Popularity",
#               x = "Median Household Income",
#               y = "search Popularity"))
```

Repeat the above steps using the covid data and the ACS data.
